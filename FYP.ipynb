{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce36ee-d187-478c-ba3c-c15dddb18186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e22c20-cb6e-457b-b75c-bc1e3749ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 files belonging to 4 classes.\n",
      "Using 24 files for training.\n",
      "Detected classes: 4\n",
      "Class names: ['aziz', 'mohammed', 'rashed', 'sultan']\n",
      "Found 30 files belonging to 4 classes.\n",
      "Using 6 files for validation.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274180 (4.86 MB)\n",
      "Trainable params: 1274180 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3794 - accuracy: 0.3333 - val_loss: 1.5690 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3036 - accuracy: 0.3333 - val_loss: 1.5271 - val_accuracy: 0.1667\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2637 - accuracy: 0.4583 - val_loss: 1.3486 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.2499 - accuracy: 0.4167 - val_loss: 1.2217 - val_accuracy: 0.6667\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.3155 - accuracy: 0.2500 - val_loss: 1.2057 - val_accuracy: 0.6667\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.1747 - accuracy: 0.5417 - val_loss: 1.2194 - val_accuracy: 0.6667\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.0249 - accuracy: 0.6667 - val_loss: 1.1776 - val_accuracy: 0.6667\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.0137 - accuracy: 0.7083 - val_loss: 1.0468 - val_accuracy: 0.6667\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.9894 - accuracy: 0.6250 - val_loss: 0.8402 - val_accuracy: 0.6667\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.8491 - accuracy: 0.7917 - val_loss: 0.6796 - val_accuracy: 0.8333\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6424 - accuracy: 0.8750 - val_loss: 0.6048 - val_accuracy: 0.6667\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6523 - accuracy: 0.8750 - val_loss: 0.4915 - val_accuracy: 0.8333\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.5994 - accuracy: 0.8333 - val_loss: 0.5843 - val_accuracy: 0.6667\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5042 - accuracy: 0.8333 - val_loss: 0.3569 - val_accuracy: 0.8333\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3099 - accuracy: 0.8750 - val_loss: 0.2472 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2865 - accuracy: 0.8750 - val_loss: 0.1303 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2948 - accuracy: 0.8750 - val_loss: 0.0919 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2165 - accuracy: 0.9167 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1477 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0950 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 1.0000\n",
      "Model saved to face_recognition_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCD\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Path to your dataset folder (each subfolder corresponds to a different person)\n",
    "data_dir = \"C:\\\\Users\\\\PCD\\\\Desktop\\\\rtrain\"  # Replace with your dataset path\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "epochs = 20             # Increase epochs if needed\n",
    "validation_split = 0.2  # 20% of data will be used for validation\n",
    "seed = 123              # For reproducibility\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Load the training dataset\n",
    "# -------------------------------\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Retrieve and store class names BEFORE caching/prefetching\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"Detected classes:\", num_classes)\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Optionally save class names to a text file\n",
    "with open(\"class_names.txt\", \"w\") as f:\n",
    "    for name in class_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Load the validation dataset\n",
    "# -------------------------------\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Optimize dataset performance\n",
    "# -------------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Data Augmentation\n",
    "# -------------------------------\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Build the CNN model\n",
    "# -------------------------------\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Apply data augmentation\n",
    "    data_augmentation,\n",
    "    \n",
    "    # Rescale pixel values to [0, 1]\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    \n",
    "    # Convolutional layers\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    \n",
    "    # Flatten and fully connected layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Final layer: number of neurons = number of classes\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Compile and Train\n",
    "# -------------------------------\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fix: Build the model explicitly so that weights are created\n",
    "model.build(input_shape=(None, img_height, img_width, 3))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Save the trained model\n",
    "# -------------------------------\n",
    "model_save_path = \"face_recognition_model.h5\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9312054e-ba26-4ebb-adae-6a6042521968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: No webcam detected or cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# Load class names\n",
    "with open(\"class_names.txt\", \"r\") as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tflite.Interpreter(model_path=\"face_recognition_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "\n",
    "# Load Haar cascades\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Webcam not detected.\")\n",
    "    exit()\n",
    "\n",
    "confidence_threshold = 0.8\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame for performance (optional)\n",
    "    frame = cv2.resize(frame, (320, 240))\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Preprocess for model input\n",
    "        face_img = cv2.resize(face_roi, (64, 64))  # same as model input size\n",
    "        face_img = face_img.astype(np.float32) / 255.0\n",
    "        input_data = np.expand_dims(face_img, axis=0)\n",
    "\n",
    "        # Set tensor and run inference\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        class_id = np.argmax(prediction)\n",
    "        confidence = np.max(prediction)\n",
    "\n",
    "        if confidence >= confidence_threshold:\n",
    "            label = class_names[class_id]\n",
    "            color = (0, 255, 0)\n",
    "        else:\n",
    "            label = \"Unknown\"\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # Smile detection\n",
    "        smiles = smile_cascade.detectMultiScale(face_gray, scaleFactor=1.8, minNeighbors=20)\n",
    "        smile_label = \"Smiling\" if len(smiles) > 0 else \"Not Smiling\"\n",
    "\n",
    "        # Draw results\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame, f\"{label}: {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        cv2.putText(frame, smile_label, (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition (TFLite)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f664bd6-c719-4b65-8a56-427bb09c54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PCD\\AppData\\Local\\Temp\\tmp1gt3iplw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PCD\\AppData\\Local\\Temp\\tmp1gt3iplw\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the .h5 model\n",
    "model = tf.keras.models.load_model(\"face_recognition_model.h5\")\n",
    "\n",
    "# Convert to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Optional for size/speed\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the .tflite model\n",
    "with open(\"face_recognition_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ Conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f3c97-e3b9-4ed5-8a89-fef57436416a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
